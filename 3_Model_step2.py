"""
XGBoost Risk Prediction Model - Step 2: Basic Evaluation Framework
================================================================

Step 2 Implementation:
- Implement proper multi-class evaluation metrics
- Create confusion matrix visualization
- Document baseline performance for comparison
- Goal: Understand current model performance

Design Focus:
- Reuse Step 1 models for pure evaluation analysis
- High-risk focused metrics (precision/recall for risk detection)
- Business-meaningful performance interpretation
- Comprehensive baseline documentation for future comparison
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    mean_absolute_error, accuracy_score, classification_report,
    confusion_matrix, precision_recall_fscore_support,
    cohen_kappa_score, balanced_accuracy_score
)
import xgboost as xgb
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# Set style for better plots
plt.style.use('default')
sns.set_palette("husl")


class AdvancedEvaluationFramework:
    """
    Step 2: Advanced Evaluation Framework for Risk Prediction Models
    
    Provides comprehensive evaluation of Step 1 baseline models with focus on:
    - Multi-class evaluation metrics
    - High-risk detection performance
    - Confusion matrix analysis
    - Business-meaningful performance interpretation
    """
    
    def __init__(self, config: Dict):
        """
        Initialize evaluation framework
        
        Args:
            config: Dictionary containing evaluation configuration
        """
        self.config = config
        self.data = None
        self.models = {}
        self.predictions = {}
        self.evaluation_results = {}
        self.feature_columns = None
        
        # Risk level definitions for business interpretation
        self.risk_levels = {
            0: "No Risk",
            1: "Low Risk", 
            2: "Medium Risk",
            3: "High Risk"
        }
        
        print("ðŸ” Advanced Evaluation Framework Initialized")
        print(f"ðŸ“Š Target variables: {config['target_columns']}")
        print(f"ðŸŽ¯ Focus: High-risk detection performance")
    
    def load_and_train_baseline_models(self):
        """
        Load data and train baseline models (replicating Step 1)
        """
        print("\n" + "="*60)
        print("1ï¸âƒ£ LOADING DATA & TRAINING BASELINE MODELS")
        print("="*60)
        
        # Load data
        self.data = pd.read_csv(self.config['data_path'])
        print(f"âœ… Data loaded: {self.data.shape}")
        
        # Apply exclusions
        exclude_cols = self.config.get('exclude_columns', [])
        if exclude_cols:
            self.data = self.data.drop(columns=[col for col in exclude_cols if col in self.data.columns])
        
        # Separate features and targets
        target_cols = self.config['target_columns']
        feature_cols = [col for col in self.data.columns if not col.startswith('risk_year')]
        self.feature_columns = feature_cols
        
        X = self.data[feature_cols]
        
        # Train/test split
        X_train, X_test = train_test_split(
            X, test_size=self.config['test_size'], 
            random_state=self.config['random_state'], shuffle=True
        )
        
        # Split targets
        y_train_dict = {}
        y_test_dict = {}
        
        for target in target_cols:
            if target in self.data.columns:
                y_target = self.data[target].values
                y_train_target, y_test_target = train_test_split(
                    y_target, test_size=self.config['test_size'],
                    random_state=self.config['random_state'], shuffle=True
                )
                y_train_dict[target] = y_train_target
                y_test_dict[target] = y_test_target
        
        # Train models
        print(f"\nðŸ”§ Training {len(target_cols)} baseline models...")
        models = {}
        predictions = {}
        
        xgb_params = {
            'objective': 'reg:squarederror',
            'enable_missing': True,
            'random_state': 42,
            'verbosity': 0
        }
        
        for target_name, y_train_values in y_train_dict.items():
            print(f"   ðŸ“ˆ Training {target_name}...")
            
            # Filter for valid data
            valid_mask = ~pd.isna(y_train_values)
            X_train_filtered = X_train[valid_mask]
            y_train_filtered = y_train_values[valid_mask]
            
            if len(y_train_filtered) == 0:
                continue
            
            # Train model
            model = xgb.XGBRegressor(**xgb_params)
            model.fit(X_train_filtered, y_train_filtered)
            models[target_name] = model
            
            # Generate predictions for evaluation
            y_test_values = y_test_dict[target_name]
            test_valid_mask = ~pd.isna(y_test_values)
            X_test_filtered = X_test[test_valid_mask]
            y_test_filtered = y_test_values[test_valid_mask]
            
            if len(y_test_filtered) > 0:
                y_pred_raw = model.predict(X_test_filtered)
                y_pred_rounded = np.clip(np.round(y_pred_raw), 0, 3).astype(int)
                
                predictions[target_name] = {
                    'raw': y_pred_raw,
                    'rounded': y_pred_rounded,
                    'actual': y_test_filtered.astype(int)
                }
        
        self.models = models
        self.predictions = predictions
        print(f"âœ… Training completed: {len(models)} models ready for evaluation")
    
    def calculate_comprehensive_metrics(self) -> Dict:
        """
        Calculate comprehensive evaluation metrics for each model
        
        Returns:
            Dictionary of detailed metrics for each target
        """
        print("\n" + "="*60)
        print("2ï¸âƒ£ COMPREHENSIVE METRICS CALCULATION")
        print("="*60)
        
        evaluation_results = {}
        
        for target_name, pred_data in self.predictions.items():
            print(f"\nðŸ“Š Evaluating {target_name}...")
            
            y_true = pred_data['actual']
            y_pred = pred_data['rounded']
            y_pred_raw = pred_data['raw']
            
            # Basic metrics
            mae_raw = mean_absolute_error(y_true, y_pred_raw)
            mae_rounded = mean_absolute_error(y_true, y_pred)
            accuracy = accuracy_score(y_true, y_pred)
            balanced_acc = balanced_accuracy_score(y_true, y_pred)
            kappa = cohen_kappa_score(y_true, y_pred)
            
            # Class-wise metrics
            precision, recall, f1, support = precision_recall_fscore_support(
                y_true, y_pred, labels=[0, 1, 2, 3], zero_division=0
            )
            
            # High-risk focused metrics (risk levels 2 and 3)
            high_risk_mask = (y_true >= 2)
            high_risk_pred_mask = (y_pred >= 2)
            
            # High-risk detection metrics
            if np.any(high_risk_mask):
                high_risk_precision = precision_recall_fscore_support(
                    high_risk_mask, high_risk_pred_mask, average='binary', zero_division=0
                )[0]
                high_risk_recall = precision_recall_fscore_support(
                    high_risk_mask, high_risk_pred_mask, average='binary', zero_division=0
                )[1]
                high_risk_f1 = precision_recall_fscore_support(
                    high_risk_mask, high_risk_pred_mask, average='binary', zero_division=0
                )[2]
            else:
                high_risk_precision = high_risk_recall = high_risk_f1 = 0.0
            
            # Class distribution analysis
            true_dist = pd.Series(y_true).value_counts().sort_index()
            pred_dist = pd.Series(y_pred).value_counts().sort_index()
            
            # Store comprehensive results
            evaluation_results[target_name] = {
                'basic_metrics': {
                    'mae_raw': mae_raw,
                    'mae_rounded': mae_rounded,
                    'accuracy': accuracy,
                    'balanced_accuracy': balanced_acc,
                    'cohen_kappa': kappa
                },
                'class_wise_metrics': {
                    'precision': precision,
                    'recall': recall,
                    'f1_score': f1,
                    'support': support
                },
                'high_risk_metrics': {
                    'precision': high_risk_precision,
                    'recall': high_risk_recall,
                    'f1_score': high_risk_f1
                },
                'distributions': {
                    'true_distribution': dict(true_dist),
                    'predicted_distribution': dict(pred_dist)
                },
                'sample_size': len(y_true)
            }
            
            # Print summary
            print(f"   ðŸ“ˆ Accuracy: {accuracy:.4f}")
            print(f"   ðŸ“ˆ Balanced Accuracy: {balanced_acc:.4f}")
            print(f"   ðŸŽ¯ High-Risk Precision: {high_risk_precision:.4f}")
            print(f"   ðŸŽ¯ High-Risk Recall: {high_risk_recall:.4f}")
            print(f"   ðŸ“Š Cohen's Kappa: {kappa:.4f}")
        
        self.evaluation_results = evaluation_results
        return evaluation_results
    
    def create_confusion_matrices(self):
        """
        Create detailed confusion matrix visualizations
        """
        print("\n" + "="*60)
        print("3ï¸âƒ£ CONFUSION MATRIX ANALYSIS")
        print("="*60)
        
        n_models = len(self.predictions)
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Confusion Matrix Analysis - Step 2 Evaluation', fontsize=16, fontweight='bold')
        
        axes = axes.flatten()
        
        for i, (target_name, pred_data) in enumerate(self.predictions.items()):
            if i >= 4:
                break
            
            y_true = pred_data['actual']
            y_pred = pred_data['rounded']
            
            # Calculate confusion matrix
            cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3])
            
            # Normalize confusion matrix
            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            
            # Create heatmap
            ax = axes[i]
            sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',
                       xticklabels=[self.risk_levels[j] for j in [0,1,2,3]],
                       yticklabels=[self.risk_levels[j] for j in [0,1,2,3]],
                       ax=ax)
            
            ax.set_title(f'{target_name} - Confusion Matrix (Normalized)')
            ax.set_xlabel('Predicted Risk Level')
            ax.set_ylabel('Actual Risk Level')
            
            # Print confusion matrix analysis
            print(f"\nðŸ“Š {target_name} Confusion Matrix Analysis:")
            print("   Raw counts:")
            for actual in range(4):
                for predicted in range(4):
                    if cm[actual, predicted] > 0:
                        print(f"     Actual {self.risk_levels[actual]} â†’ Predicted {self.risk_levels[predicted]}: {cm[actual, predicted]}")
        
        plt.tight_layout()
        plt.show()
    
    def analyze_error_patterns(self):
        """
        Analyze detailed error patterns across models
        """
        print("\n" + "="*60)
        print("4ï¸âƒ£ ERROR PATTERN ANALYSIS")
        print("="*60)
        
        # Create error analysis visualization
        fig, axes = plt.subplots(2, 2, figsize=(16, 10))
        fig.suptitle('Error Pattern Analysis - Business Impact Focus', fontsize=16, fontweight='bold')
        
        axes = axes.flatten()
        
        for i, (target_name, pred_data) in enumerate(self.predictions.items()):
            if i >= 4:
                break
            
            y_true = pred_data['actual']
            y_pred = pred_data['rounded']
            
            # Calculate prediction errors
            errors = y_pred - y_true
            
            # Create error distribution plot
            ax = axes[i]
            error_counts = pd.Series(errors).value_counts().sort_index()
            
            colors = ['red' if x < 0 else 'orange' if x > 0 else 'green' for x in error_counts.index]
            bars = ax.bar(error_counts.index, error_counts.values, color=colors, alpha=0.7)
            
            ax.set_title(f'{target_name} - Prediction Errors')
            ax.set_xlabel('Error (Predicted - Actual)')
            ax.set_ylabel('Count')
            ax.grid(True, alpha=0.3)
            
            # Add error interpretation
            underestimate = (errors < 0).sum()  # Predicted lower than actual (dangerous)
            overestimate = (errors > 0).sum()   # Predicted higher than actual (conservative)
            perfect = (errors == 0).sum()       # Perfect predictions
            
            ax.text(0.02, 0.98, 
                   f'Perfect: {perfect}\nUnder-estimate: {underestimate}\nOver-estimate: {overestimate}',
                   transform=ax.transAxes, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        plt.tight_layout()
        plt.show()
        
        # Print business impact analysis
        print(f"\nðŸ’¼ BUSINESS IMPACT ANALYSIS:")
        for target_name, pred_data in self.predictions.items():
            y_true = pred_data['actual']
            y_pred = pred_data['rounded']
            errors = y_pred - y_true
            
            # Dangerous errors (underestimating risk)
            dangerous_errors = (errors < 0).sum()
            total_high_risk = (y_true >= 2).sum()
            missed_high_risk = ((y_true >= 2) & (y_pred < 2)).sum()
            
            print(f"\n   ðŸŽ¯ {target_name}:")
            print(f"     â€¢ Total high-risk cases: {total_high_risk}")
            print(f"     â€¢ Missed high-risk cases: {missed_high_risk}")
            print(f"     â€¢ High-risk detection rate: {(1 - missed_high_risk/max(total_high_risk, 1)):.2%}")
            print(f"     â€¢ Dangerous underestimations: {dangerous_errors}")
    
    def create_performance_comparison(self):
        """
        Create comprehensive performance comparison across models
        """
        print("\n" + "="*60)
        print("5ï¸âƒ£ PERFORMANCE COMPARISON ACROSS MODELS")
        print("="*60)
        
        # Prepare comparison data
        comparison_data = []
        for target_name, results in self.evaluation_results.items():
            comparison_data.append({
                'Model': target_name,
                'Accuracy': results['basic_metrics']['accuracy'],
                'Balanced_Acc': results['basic_metrics']['balanced_accuracy'],
                'MAE': results['basic_metrics']['mae_rounded'],
                'Cohen_Kappa': results['basic_metrics']['cohen_kappa'],
                'High_Risk_Precision': results['high_risk_metrics']['precision'],
                'High_Risk_Recall': results['high_risk_metrics']['recall'],
                'High_Risk_F1': results['high_risk_metrics']['f1_score'],
                'Sample_Size': results['sample_size']
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        
        # Create comparison visualization
        fig, axes = plt.subplots(2, 2, figsize=(16, 10))
        fig.suptitle('Model Performance Comparison - Step 2 Baseline', fontsize=16, fontweight='bold')
        
        # 1. Overall performance metrics
        ax1 = axes[0, 0]
        x_pos = np.arange(len(comparison_df))
        width = 0.25
        
        ax1.bar(x_pos - width, comparison_df['Accuracy'], width, label='Accuracy', alpha=0.8)
        ax1.bar(x_pos, comparison_df['Balanced_Acc'], width, label='Balanced Acc', alpha=0.8)
        ax1.bar(x_pos + width, comparison_df['Cohen_Kappa'], width, label='Cohen Kappa', alpha=0.8)
        
        ax1.set_title('Overall Performance Metrics')
        ax1.set_ylabel('Score')
        ax1.set_xticks(x_pos)
        ax1.set_xticklabels(comparison_df['Model'], rotation=45)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. High-risk detection performance
        ax2 = axes[0, 1]
        ax2.bar(x_pos - width/2, comparison_df['High_Risk_Precision'], width, label='Precision', alpha=0.8)
        ax2.bar(x_pos + width/2, comparison_df['High_Risk_Recall'], width, label='Recall', alpha=0.8)
        
        ax2.set_title('High-Risk Detection Performance')
        ax2.set_ylabel('Score')
        ax2.set_xticks(x_pos)
        ax2.set_xticklabels(comparison_df['Model'], rotation=45)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. Sample sizes
        ax3 = axes[1, 0]
        ax3.bar(comparison_df['Model'], comparison_df['Sample_Size'], alpha=0.8)
        ax3.set_title('Test Sample Sizes')
        ax3.set_ylabel('Number of Samples')
        ax3.tick_params(axis='x', rotation=45)
        ax3.grid(True, alpha=0.3)
        
        # 4. MAE comparison
        ax4 = axes[1, 1]
        ax4.bar(comparison_df['Model'], comparison_df['MAE'], alpha=0.8, color='orange')
        ax4.set_title('Mean Absolute Error')
        ax4.set_ylabel('MAE')
        ax4.tick_params(axis='x', rotation=45)
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # Print detailed comparison table
        print(f"\nðŸ“Š DETAILED PERFORMANCE COMPARISON TABLE:")
        print("-" * 80)
        
        for _, row in comparison_df.iterrows():
            print(f"\nðŸŽ¯ {row['Model']}:")
            print(f"   â€¢ Accuracy: {row['Accuracy']:.4f}")
            print(f"   â€¢ Balanced Accuracy: {row['Balanced_Acc']:.4f}")
            print(f"   â€¢ MAE: {row['MAE']:.4f}")
            print(f"   â€¢ Cohen's Kappa: {row['Cohen_Kappa']:.4f}")
            print(f"   â€¢ High-Risk Precision: {row['High_Risk_Precision']:.4f}")
            print(f"   â€¢ High-Risk Recall: {row['High_Risk_Recall']:.4f}")
            print(f"   â€¢ High-Risk F1: {row['High_Risk_F1']:.4f}")
            print(f"   â€¢ Test Sample Size: {row['Sample_Size']:,}")
    
    def generate_baseline_documentation(self):
        """
        Generate comprehensive baseline documentation for future comparison
        """
        print("\n" + "="*60)
        print("6ï¸âƒ£ BASELINE DOCUMENTATION GENERATION")
        print("="*60)
        
        # Create comprehensive summary
        summary = {
            'step2_evaluation_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
            'model_configuration': {
                'algorithm': 'XGBoost Regression',
                'approach': 'Separate models per year',
                'target_encoding': 'Regression with rounding',
                'missing_value_handling': 'Native XGBoost',
                'train_test_split': f"{int((1-self.config['test_size'])*100)}/{int(self.config['test_size']*100)}"
            },
            'performance_summary': {},
            'key_findings': {},
            'recommendations_for_next_steps': []
        }
        
        # Performance summary
        for target_name, results in self.evaluation_results.items():
            summary['performance_summary'][target_name] = {
                'accuracy': results['basic_metrics']['accuracy'],
                'balanced_accuracy': results['basic_metrics']['balanced_accuracy'],
                'mae': results['basic_metrics']['mae_rounded'],
                'high_risk_recall': results['high_risk_metrics']['recall'],
                'sample_size': results['sample_size'],
                'class_distribution': results['distributions']['true_distribution']
            }
        
        # Key findings
        accuracies = [r['basic_metrics']['accuracy'] for r in self.evaluation_results.values()]
        high_risk_recalls = [r['high_risk_metrics']['recall'] for r in self.evaluation_results.values()]
        
        best_model = max(self.evaluation_results.keys(), 
                        key=lambda x: self.evaluation_results[x]['basic_metrics']['accuracy'])
        worst_model = min(self.evaluation_results.keys(), 
                         key=lambda x: self.evaluation_results[x]['basic_metrics']['accuracy'])
        
        summary['key_findings'] = {
            'best_performing_model': best_model,
            'worst_performing_model': worst_model,
            'average_accuracy': np.mean(accuracies),
            'average_high_risk_recall': np.mean(high_risk_recalls),
            'performance_consistency': np.std(accuracies),
            'class_imbalance_impact': 'Year1 most affected by severe imbalance'
        }
        
        # Recommendations
        summary['recommendations_for_next_steps'] = [
            'Address class imbalance especially in Year1 model',
            'Consider temporal validation to avoid optimistic bias',
            'Investigate model architecture (separate vs joint)',
            'Focus feature engineering on macro-economic indicators',
            'Implement cost-sensitive training for high-risk detection'
        ]
        
        # Save documentation
        import json
        with open('step2_baseline_documentation.json', 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        
        print("âœ… Baseline documentation saved to 'step2_baseline_documentation.json'")
        
        # Print executive summary
        print(f"\nðŸ“‹ EXECUTIVE SUMMARY - STEP 2 BASELINE EVALUATION:")
        print("=" * 60)
        print(f"ðŸ“… Evaluation Date: {summary['step2_evaluation_date']}")
        print(f"ðŸŽ¯ Best Model: {best_model} (Accuracy: {self.evaluation_results[best_model]['basic_metrics']['accuracy']:.4f})")
        print(f"âš ï¸  Worst Model: {worst_model} (Accuracy: {self.evaluation_results[worst_model]['basic_metrics']['accuracy']:.4f})")
        print(f"ðŸ“Š Average Accuracy: {np.mean(accuracies):.4f}")
        print(f"ðŸŽ¯ Average High-Risk Recall: {np.mean(high_risk_recalls):.4f}")
        print(f"ðŸ“ˆ Performance Consistency (std): {np.std(accuracies):.4f}")
        
        print(f"\nðŸ” KEY INSIGHTS:")
        print("â€¢ Class imbalance severely impacts Year1 model performance")
        print("â€¢ High-risk detection varies significantly across models")
        print("â€¢ Current approach provides reasonable baseline")
        print("â€¢ Major improvements needed in high-risk recall")
        
        print(f"\nâž¡ï¸  READY FOR STEP 3: Comprehensive EDA")
    
    def run_step2_evaluation(self):
        """
        Execute complete Step 2 evaluation pipeline
        """
        print("ðŸ” EXECUTING STEP 2: BASIC EVALUATION FRAMEWORK")
        print("=" * 70)
        
        try:
            # Load and train baseline models
            self.load_and_train_baseline_models()
            
            # Calculate comprehensive metrics
            self.calculate_comprehensive_metrics()
            
            # Create confusion matrices
            self.create_confusion_matrices()
            
            # Analyze error patterns
            self.analyze_error_patterns()
            
            # Create performance comparison
            self.create_performance_comparison()
            
            # Generate baseline documentation
            self.generate_baseline_documentation()
            
            print("\nðŸŽ‰ STEP 2 COMPLETED SUCCESSFULLY!")
            print("âœ… Comprehensive evaluation framework established")
            print("âœ… Baseline performance documented")
            print("âœ… Ready for Step 3: Comprehensive EDA")
            
        except Exception as e:
            print(f"\nâŒ STEP 2 FAILED: {e}")
            raise


def get_step2_config():
    """
    Configuration for Step 2 evaluation framework
    """
    return {
        'data_path': 'dataset/credit_risk_dataset.csv',
        'exclude_columns': [
            'ì‚¬ì—…ìžë“±ë¡ë²ˆí˜¸',
            'ëŒ€ìƒìžëª…',
            'ëŒ€ìƒìžë“±ë¡ì´ë ¥ì¼ì‹œ',
            'ëŒ€ìƒìžê¸°ë³¸ì£¼ì†Œ',
            'ì²­ì•½ë²ˆí˜¸',
            'ë³´í—˜ì²­ì•½ì¼ìž',
            'ì²­ì•½ìƒíƒœì½”ë“œ',
            'ìˆ˜ì¶œìžëŒ€ìƒìžë²ˆí˜¸',
            'íŠ¹ë³„ì¶œì—°í˜‘ì•½ì½”ë“œ',
            'ì—…ì¢…ì½”ë“œ1'
        ],
        'target_columns': ['risk_year1', 'risk_year2', 'risk_year3', 'risk_year4'],
        'test_size': 0.2,
        'random_state': 42
    }


# Main execution
if __name__ == "__main__":
    
    print("ðŸ” Starting XGBoost Risk Prediction Model - Step 2")
    print("="*60)
    
    # Get configuration
    config = get_step2_config()
    
    # Create and run evaluation framework
    evaluator = AdvancedEvaluationFramework(config)
    evaluator.run_step2_evaluation()
    
    print("\nðŸ Step 2 execution completed!")
    print("Ready to proceed to Step 3: Comprehensive EDA")