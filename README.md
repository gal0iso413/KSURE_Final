# KSURE Credit Risk Model Pipeline

## ğŸ“‹ Project Overview

This is a comprehensive credit risk prediction model for KSURE (Korea Trade Insurance Corporation). The model predicts company default risk over 1-4 year horizons using financial data, trade statistics, credit ratings, and macroeconomic indicators.

## ğŸ—ï¸ Project Structure

```
KSURE_Final/
â”œâ”€â”€ ğŸ“ src/                             # All Python source code
â”‚   â”œâ”€â”€ 1_Dataset.py                    # â­ START HERE: Dataset creation
â”‚   â”œâ”€â”€ 1_Split.py                      # Data splitting (train/validation/OOT)
â”‚   â”‚
â”‚   â”œâ”€â”€ 2_Model1_baseline.py            # ğŸ” COMPARISON MODELS (Optional)
â”‚   â”œâ”€â”€ 2_Model1_evaluate.py            # Model evaluation
â”‚   â”œâ”€â”€ 2_Model1_explore.py             # Model exploration
â”‚   â”œâ”€â”€ 2_Model2.py                     # Feature reduction experiments
â”‚   â”œâ”€â”€ 2_Model3.py                     # Temporal validation experiments
â”‚   â”œâ”€â”€ 2_Model4.py                     # Model comparison experiments
â”‚   â”œâ”€â”€ 2_Model5.py                     # Class imbalance experiments
â”‚   â”œâ”€â”€ 2_Model6.py                     # Architecture experiments
â”‚   â”œâ”€â”€ 2_Model7.py                     # Hyperparameter experiments
â”‚   â”‚
â”‚   â”œâ”€â”€ 3_Model8.py                     # ğŸ¯ MAIN PIPELINE: Advanced modeling
â”‚   â”œâ”€â”€ 4_Model9.py                     # Final model training
â”‚   â”œâ”€â”€ 5_Model10.py                    # Production model & grading
â”‚   â”‚
â”‚   â”œâ”€â”€ external_data.py                # External data processing utilities
â”‚   â”œâ”€â”€ ì¬ë¬´ì œí‘œ_process.py             # Financial statement processing
â”‚   â””â”€â”€ ì¡°ê¸°ê²½ë³´_process.py             # Early warning processing
â”‚
â”œâ”€â”€ ğŸ“ data/                            # All data files
â”‚   â”œâ”€â”€ raw/                            # Original source data
â”‚   â”‚   â”œâ”€â”€ ì²­ì•½.csv                    # Insurance contracts (base table)
â”‚   â”‚   â”œâ”€â”€ ì¡°ê¸°ê²½ë³´ì´ë ¥_ë¦¬ìŠ¤í¬ë‹¨ê³„.csv  # Risk outcomes (Y variables)
â”‚   â”‚   â”œâ”€â”€ KEDê°€ê³µì¬ë¬´DATA.csv         # Financial data
â”‚   â”‚   â”œâ”€â”€ KEDì¢…í•©ì‹ ìš©ì •ë³´.csv         # Credit ratings
â”‚   â”‚   â”œâ”€â”€ ë¬´ì—­í†µê³„ì§„í¥ì›ìˆ˜ì¶œì…ì‹¤ì .csv # Trade statistics
â”‚   â”‚   â”œâ”€â”€ ì—…ì¢…ì½”ë“œ_ìˆ˜ì¶œì.csv         # Industry codes
â”‚   â”‚   â”œâ”€â”€ gdp_data.csv                # GDP data
â”‚   â”‚   â”œâ”€â”€ trade_data.csv              # Trade data
â”‚   â”‚   â””â”€â”€ exchange_rate_data.csv      # Exchange rate data
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/                      # Processed datasets
â”‚   â”‚   â”œâ”€â”€ credit_risk_dataset.csv     # Main dataset from 1_Dataset.py
â”‚   â”‚   â””â”€â”€ credit_risk_dataset_selected.csv # Selected features
â”‚   â”‚
â”‚   â””â”€â”€ splits/                         # Data splits from 1_Split.py
â”‚       â”œâ”€â”€ train_data.csv              # Training data (part of development)
â”‚       â”œâ”€â”€ validation_data.csv         # Validation data (part of development)
â”‚       â”œâ”€â”€ oot_data.csv                # Test data (final evaluation)
â”‚       â””â”€â”€ split_metadata.json         # Split information
â”‚
â”œâ”€â”€ ğŸ“ models/                          # Trained models
â”‚   â”œâ”€â”€ baseline/                       # Models from 2_Model* experiments
â”‚   â”œâ”€â”€ intermediate/                   # Models from development steps
â”‚   â””â”€â”€ final/                          # Production-ready models
â”‚
â”œâ”€â”€ ğŸ“ results/                         # All outputs and results
â”‚   â”œâ”€â”€ step1_baseline/                 # Baseline model results
â”‚   â”œâ”€â”€ step2_feature_reduction/        # Feature selection results
â”‚   â”œâ”€â”€ step3_temporal_validation/      # Temporal validation results
â”‚   â”œâ”€â”€ step4_model_comparison/         # Model comparison results
â”‚   â”œâ”€â”€ step5_class_imbalance/          # Class imbalance handling results
â”‚   â”œâ”€â”€ step6_model_architecture/       # Architecture comparison results
â”‚   â”œâ”€â”€ step7_optuna/                   # Hyperparameter optimization results
â”‚   â”œâ”€â”€ step8_post/                     # Advanced modeling results
â”‚   â”œâ”€â”€ validation/                     # Model validation results
â”‚   â””â”€â”€ grading/                        # Final grading and predictions
â”‚
â”œâ”€â”€ ğŸ“ sql/                             # SQL scripts for data extraction
â”‚   â”œâ”€â”€ KEDì¬ë¬´DATA.sql
â”‚   â”œâ”€â”€ KEDì¢…í•©ì‹ ìš©ì •ë³´.sql
â”‚   â”œâ”€â”€ ë¬´ì—­í†µê³„ì§„í¥ì›ìˆ˜ì¶œì…ì‹¤ì _test.sql
â”‚   â”œâ”€â”€ ì¡°ê¸°ê²½ë³´ë‚´ì—­.sql
â”‚   â””â”€â”€ ì²­ì•½_test.sql
â”‚
â”œâ”€â”€ requirements.txt                    # Python dependencies
â””â”€â”€ README.md                          # This file
```

## ğŸš€ How to Run the Pipeline

### **MAIN PIPELINE (Recommended)**

For production use, run the main pipeline in this order:

```bash
cd src/

# Step 1: Data Preparation
python 1_Dataset.py          # Create comprehensive dataset
python 1_Split.py             # Split data (train/validation/OOT)

# Step 2: Feature Selection & Model Development
python 2_Model2.py            # Feature selection (development/test split)

# Step 3: Main Model Pipeline
python 3_Model8.py            # Advanced modeling techniques
python 4_Model9.py            # Final model training
python 5_Model10.py           # Production model & grade assignment
```

### **EXPERIMENTAL PIPELINE (Optional)**

If you want to explore different modeling approaches:

```bash
cd src/

# After running 1_Dataset.py and 1_Split.py...

# Model Development Experiments (Optional)
python 2_Model2.py            # Feature selection (development/test split)
python 2_Model3.py            # Temporal validation experiments
python 2_Model4.py            # Model comparison experiments
python 2_Model5.py            # Class imbalance handling experiments
python 2_Model6.py            # Architecture comparison experiments
python 2_Model7.py            # Hyperparameter optimization experiments

# Then continue with main pipeline (3_Model8.py â†’ 4_Model9.py â†’ 5_Model10.py)
```

## ğŸ“Š Key Features

### **Data Sources**
- **Insurance Contracts**: Base table with contract information
- **Risk Outcomes**: Historical default/risk events (1-4 year horizons)
- **Financial Data**: Balance sheet and income statement ratios
- **Credit Ratings**: External credit assessment data
- **Trade Statistics**: Import/export performance data
- **Macroeconomic Data**: GDP, exchange rates, trade volumes
- **Industry Classifications**: Sector-specific risk factors

### **Model Architecture**
- **Multi-horizon Prediction**: Separate risk predictions for 1, 2, 3, and 4 years
- **Temporal Validation**: Development/test split with strict time-based validation to prevent leakage
- **Feature Engineering**: Lookback periods, change rates, and temporal aggregations
- **Class Imbalance Handling**: Advanced techniques for rare default events
- **Ensemble Methods**: Multiple algorithms with optimal weighting

### **Risk Grading System**
- **10-Grade Scale**: AAA (lowest risk) to D (highest risk)
- **Monotonic Grading**: Higher grades correspond to higher observed default rates
- **Business Rules**: Interpretable grade assignment logic
- **Validation Framework**: Out-of-time testing for unbiased performance assessment

## ğŸ¯ Key Outputs

After running the pipeline, you will have:

1. **Trained Models**: Production-ready XGBoost models for each time horizon
2. **Risk Grades**: A-D grade assignments for all companies
3. **Predictions**: Risk scores and probabilities for each company
4. **Validation Reports**: Comprehensive model performance analysis
5. **Business Insights**: Interpretable risk factors and grade explanations

## âš ï¸ Important Notes

### **Current Status**
- âœ… Files have been reorganized into logical directory structure
- âœ… **Development/Test split methodology implemented** (2_Model2.py creates clean split)
- âœ… **Models 3-7 use development data only** (filter by data_split column)
- âœ… All original functionality preserved

### **Data Requirements**
- All source data files should be placed in `data/raw/` directory
- Ensure Korean character encoding is properly handled (UTF-8)
- Verify date formats are consistent across all data sources

### **Execution Environment**
- Python 3.8+ required
- Install dependencies: `pip install -r requirements.txt`
- Recommended: 16GB+ RAM for large dataset processing
- GPU optional but recommended for faster model training

## ğŸ”§ Troubleshooting

### **Common Issues**
1. **File Path Errors**: Update file paths in scripts to reflect new directory structure
2. **Memory Issues**: Consider reducing dataset size or using batch processing
3. **Korean Font Issues**: Install appropriate Korean fonts for visualization
4. **Missing Data**: Verify all source files are present in `data/raw/`

### **Next Steps for Setup**
1. Run 1_Dataset.py and 1_Split.py to create initial data splits
2. Run 2_Model2.py to create development/test split and feature selection
3. Run Models 3-7 for model development (use development data only)
4. Run 3_Model8.py â†’ 4_Model9.py â†’ 5_Model10.py for final pipeline
5. Validate final outputs and grades

## ğŸ“ Support

This model pipeline represents a comprehensive credit risk assessment system designed for production use at KSURE. The modular design allows for easy maintenance, updates, and handover to new team members.

---

*Last Updated: 2024 - KSURE Credit Risk Modeling Team*
